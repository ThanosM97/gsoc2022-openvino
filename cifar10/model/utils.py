"""This module implements functions used by DiStyleGAN."""
import json
from pathlib import Path

import torch
import torchvision.transforms as T
from torch import nn
from torch.utils.data import DataLoader
from torchvision.transforms import Compose
from torchvision.datasets import CIFAR10

from model.dataset import FakeCIFAR10


def inverse_normalization(image: torch.Tensor) -> torch.Tensor:
    """Inverse normalization from [-1,1] to [0,1]."""
    # [-1,1] to [0,2]
    image = image + 1

    # [0,2] to [0,1]
    image = image - image.min()
    image_0_1 = image / (image.max() - image.min())

    return image_0_1


def save_images(images: torch.Tensor, path: str, dirname: str) -> None:
    """Save a single or a batch of images.

    This function takes as input a tensor containing the images to
    be saved, and saves them in the following directory:
                `path`/images/`dirname`/

    Args:
        - images (Tensor) : tensor containing the images to be saved. The 
                           tensor must have the following format:
                           (number_of_images x C x H x W)
        - path (str) : directory's path to save the images
        - dirname (str) : used for naming purposes
    """
    loc = Path(
        path,
        f'images/{dirname}'
    )
    loc.mkdir(parents=True, exist_ok=True)

    transform = T.ToPILImage()

    for i, img in enumerate(images):
        img = img.cpu()
        img = inverse_normalization(img)
        img_PIL = transform(img)

        filepath = loc / Path(f'image-{i}.png')
        img_PIL.save(filepath)


def save_checkpoints(
        netG: nn.Module, netD: nn.Module,
        optimizerG: torch.optim.Adam, optimizerD: torch.optim.Adam,
        epoch: int, path: str,
        logG: dict, logD: dict) -> None:
    """Save the Generator's and Discriminator's model states.

    This function saves the Generator and Discriminator states, along with the
    states of their corresponding optimizers and a log of their respective
    losses at that `epoach`, to the following files:

        - generator.pt
        - discriminator.pt
        - optimizerG.pt
        - optimizerD.pt
        - log.json

    inside the `path`/checkpoints/epoch-`epoch`/ directory.

    Args:
        - netG (nn.Module) : the Generator to be saved
        - netsD (nn.Module) : the Discriminator to be saved
        - epoch (int) : the current epoch, which is used for
                        naming purposes
        - path (str) : the directory's path to save the checkpoints
        - logG (dict) : dictionary of the Generator's losses
        - logD (dict) : dictionary of the Discriminator's losses
    """
    loc = Path(
        path,
        f'checkpoints/epoch-{epoch}'
    )
    loc.mkdir(parents=True, exist_ok=True)

    torch.save(
        netG.state_dict(),
        Path(loc, f'generator.pt')
    )

    torch.save(
        netD.state_dict(),
        Path(loc, f'discriminator.pt')
    )

    torch.save(
        optimizerG.state_dict(),
        Path(loc, f'optimizerG.pt')
    )

    torch.save(
        optimizerD.state_dict(),
        Path(loc, f'optimizerD.pt')
    )

    log = {
        "epoch": epoch,
        "lossG": logG,
        "lossD": logD
    }

    with open(Path(loc, "log.json"), "w") as f:
        json.dump(log, f)


def decay_lr(
        optimizer: torch.optim.Adam, max_epoch: int,
        initial_decay: int, initial_lr: float) -> None:
    """Gradually decay the `optimizer`'s learning rate.

    Args:
        - optimizer (Adam) : the optimizer whose learning rate is going to be
                             decayed
        - max_epoch (int) : the total epochs specified for the training
        - initial_decay (int) : the batch iteration at which the decay starts
        - initial_lr (float) : the initial learning rate of the optimizer
    """
    coeff = -initial_lr / (max_epoch - initial_decay)
    for pg in optimizer.param_groups:
        pg['lr'] += coeff


def get_dataloaders(
        dataset: str | Path, transform: Compose, batch_size: int = 32,
        real_dataset: str | Path | None = None, num_workers: int = 0
) -> "tuple[DataLoader, DataLoader]":
    """Return a FakeCIFAR10 DataLoader and a CIFAR10 DataLoader.

    This function returns a DataLoader for the FakeCIFAR10 dataset of the
    teacher-generated images, and a DataLoader for the original CIFAR10
    dataset.

    Args:
        - dataset (str | Path) : path to the dataset directory of the fake 
                                 CIFAR10 data generated by the teacher network
        - transform (Compose) : transform to be applied on the
                                sample images of the datasets
        - batch_size (int, optional) : number of samples per batch 
                                       (Default: 32)
        - real_dataset (str | Path | None, optional) : path to the dataset 
            directory of the real CIFAR10 data. (Default: None, it will be
            downloaded and saved in the parent directory of input `dataset` 
            path)
        - num_workers (int, optional) : number of subprocesses to use for data 
                                        loading (Default: 0, whichs means that 
                                        the data will be loaded in the main 
                                        process.)

    Returns:
        tuple[fakeCIFAR10_loader, CIFAR10_loader]
    """
    # Datasets
    fake_cifar_ds = FakeCIFAR10(dataset, transform=transform)

    if real_dataset is None:
        real_dataset = Path(dataset).parent
    cifar10_ds = CIFAR10(
        root=real_dataset,
        download=True, transform=transform)

    # Dataloaders
    fakeCIFAR_loader = DataLoader(
        fake_cifar_ds,
        batch_size=batch_size,
        shuffle=True,
        drop_last=True,
        num_workers=num_workers
    )
    cifar10_loader = DataLoader(
        cifar10_ds,
        batch_size=batch_size,
        shuffle=True,
        drop_last=True,
        num_workers=num_workers
    )

    return fakeCIFAR_loader, cifar10_loader
